{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph build and data processing\n",
    "<br>\n",
    "1) Build knowledge graph using RDFLib, based on SiriusGeoOnto format\n",
    "<br>\n",
    "2) Add classes and individuals to the knowledge graph from geolit_dv.csv\n",
    "<br>\n",
    "3) Extract triples from turtle file and remove URI\n",
    "<br>\n",
    "4) Knowledge graph metrics\n",
    "<br>\n",
    "5) Split data to evaluation into train, validation, test\n",
    "<br>\n",
    "6) Extract entites and relations for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Build knowledge graph using RDFLib, based on SiriusGeoOnto format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Literal, RDF, URIRef, BNode, Literal, Namespace\n",
    "from rdflib.namespace import FOAF, XSD, RDF, RDFS, DCTERMS, OWL\n",
    "\n",
    "# Initialise graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import namespace from SiriusGeoOnto to use are URI\n",
    "n = Namespace('http://no.sirius.ontology/geological-ontology#')\n",
    "g.bind('', n)\n",
    "g.bind('rdf', RDF)\n",
    "g.bind('terms', DCTERMS)\n",
    "g.bind('owl', OWL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object properties\n",
    "\n",
    "hasNext = n['hasNext']\n",
    "g.add((hasNext, RDF.type, OWL.ObjectProperty))        \n",
    "\n",
    "hasDirectNext = n['hasDirectNext']\n",
    "g.add((hasDirectNext, RDF.type, OWL.ObjectProperty))\n",
    "g.add((hasDirectNext, RDFS.subPropertyOf, hasNext))\n",
    "\n",
    "hasPrevious = n['hasPrevious']\n",
    "g.add((hasPrevious, RDF.type, OWL.ObjectProperty))  \n",
    "g.add((hasPrevious, OWL.inverseOf, hasNext))  \n",
    "\n",
    "hasDirectPrevious = n['hasDirectPrevious']\n",
    "g.add((hasDirectPrevious, RDF.type, OWL.ObjectProperty))\n",
    "g.add((hasDirectNext, RDFS.subPropertyOf, hasDirectPrevious))\n",
    "g.add((hasDirectPrevious, OWL.inverseOf, hasDirectNext)) \n",
    "\n",
    "isMemberOf = n['isMemberOf']\n",
    "g.add((isMemberOf, RDF.type, OWL.ObjectProperty))\n",
    "\n",
    "depositedIn = n['depositedIn']\n",
    "g.add((depositedIn, RDF.type, OWL.ObjectProperty))\n",
    "\n",
    "constitutedBy = n['constitutedBy']\n",
    "g.add((constitutedBy, RDF.type, OWL.ObjectProperty))\n",
    "\n",
    "hasAge = n['hasAge']\n",
    "g.add((hasAge, RDF.type, OWL.ObjectProperty))\n",
    "\n",
    "contains = n['contains']\n",
    "g.add((contains, RDF.type, OWL.ObjectProperty))\n",
    "\n",
    "locatedIn= n['locatedIn']\n",
    "g.add((locatedIn, RDF.type, OWL.ObjectProperty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depositional environment class and subclasses\n",
    "\n",
    "DepositionalEnvironment = n['DepositionalEnvironment']\n",
    "g.add((DepositionalEnvironment, RDF.type, OWL.Class))\n",
    "\n",
    "DepositionalEnvironmentSub = ['ContinentalDepositionalEnvironment', 'GlacialDepositionalEnvironment', 'MarineDepositionalEnvironment', 'TransitionalDepositionalEnvironment', 'VolcanicDepositionalEnvironment']\n",
    "\n",
    "for className in DepositionalEnvironmentSub:\n",
    "    className = n[className]\n",
    "    g.add((className, RDF.type, OWL.Class))\n",
    "    g.add((className, RDFS.subClassOf, DepositionalEnvironment))\n",
    "\n",
    "    ContinentalDepositionalEnvironment = n['ContinentalDepositionalEnvironment']\n",
    "    ContinentalDepositionalEnvironmentSub = ['AeolianDepositionalEnvironment', 'AlluvialDepositionalEnvironment', 'FluvialDepositionalEnvironment', 'LacustrineDepositionalEnvironment']\n",
    "    for className in ContinentalDepositionalEnvironmentSub:\n",
    "        className = n[className]\n",
    "        g.add((className, RDF.type, OWL.Class))\n",
    "        g.add((className, RDFS.subClassOf, ContinentalDepositionalEnvironment))\n",
    "\n",
    "        AeolianDepositionalEnvironment = n['AeolianDepositionalEnvironment']\n",
    "        AeolianDepositionalEnvironmentSub = ['AeolianDepositionalEnv']\n",
    "        for item in AeolianDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, AeolianDepositionalEnvironment))\n",
    "\n",
    "        AlluvialDepositionalEnvironment = n['AlluvialDepositionalEnvironment']\n",
    "        AlluvialDepositionalEnvironmentSub = ['AlluvialChannel', 'AlluvialDepositionalEnv', 'AlluvialFan']\n",
    "        for item in AlluvialDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, AlluvialDepositionalEnvironment))\n",
    "\n",
    "        FluvialDepositionalEnvironment = n['FluvialDepositionalEnvironment']\n",
    "        FluvialDepositionalEnvironmentSub = ['FluvialChannel', 'FluvialDepositionalEnv', 'FluvioDeltaicDepositionalEnvironment', 'FluvioLacustrineDepositionalEnv']\n",
    "        for item in FluvialDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, FluvialDepositionalEnvironment))\n",
    "\n",
    "        LacustrineDepositionalEnvironment = n['LacustrineDepositionalEnvironment']\n",
    "        LacustrineDepositionalEnvironmentSub = ['FluvioLacustrineDepositionalEnvironment', 'LacustrineDepositionalEnv']\n",
    "        for item in LacustrineDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, LacustrineDepositionalEnvironment))\n",
    "    \n",
    "## Marine Depositonal Environment\n",
    "    MarineDepositionalEnvironment = n['MarineDepositionalEnvironment']\n",
    "    MarineDepositionalEnvSub = ['CoastalToShallowMarineDepositionalEnvironment', 'InnerShelfDepositionalEnvironment', 'MarineDepositionalEnv', \n",
    "                                 'OpenMarine', 'OuterShelfDepositionalEnvironment', 'ReefalDepositionalEnvironment', 'ShelfDepositionalEnvironment', 'SlopeDepositionalEnvironment']\n",
    "    for marineSub in MarineDepositionalEnvSub:\n",
    "        marineSub = n[marineSub]        \n",
    "        g.add((marineSub, RDF.type, OWL.NamedIndividual))\n",
    "        g.add((marineSub, RDF.type, MarineDepositionalEnvironment))\n",
    "\n",
    "    MarineDepositionalEnvironmentSub = ['DeepMarineDepositionalEnvironment', 'ShallowMarineDepositionalEnvironment']\n",
    "    for className in MarineDepositionalEnvironmentSub:\n",
    "        className = n[className]\n",
    "        g.add((className, RDF.type, OWL.Class))\n",
    "        g.add((className, RDFS.subClassOf, MarineDepositionalEnvironment))\n",
    "\n",
    "        DeepMarineDepositionalEnvironment = n['DeepMarineDepositionalEnvironment']\n",
    "        DeepMarineDepositionalEnvironmentSub = ['AbyssalChannel', 'AbyssalFan', 'DeepMarineDepositionalEnvironment', 'ShallowMarineToDeepMarineDepositionalEnvironment', 'Turbidity-flowDeposit']\n",
    "        for item in DeepMarineDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, DeepMarineDepositionalEnvironment))    \n",
    "    \n",
    "        ShallowMarineDepositionalEnvironment = n['ShallowMarineDepositionalEnvironment']\n",
    "        ShallowMarineDepositionalEnvironmentSub = ['ShallowMarineToDeepMarineDepositionalEnvironment', 'ShallowMarineDepositionalEnv']\n",
    "        for item in ShallowMarineDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, ShallowMarineDepositionalEnvironment))    \n",
    "\n",
    "##Transitional Environment         \n",
    "    TransitionalDepositionalEnvironment = n['TransitionalDepositionalEnvironment']\n",
    "    TransitionalDepositionalEnvSub = ['CoastalPlain', 'CoastalToShallowMarineDepositionalEnvironment', 'LowerCoastalPlain', 'UpperCoastalPlain']\n",
    "    for transitionalSub in TransitionalDepositionalEnvSub:\n",
    "        transitionalSub = n[transitionalSub]  \n",
    "        g.add((transitionalSub, RDF.type, OWL.NamedIndividual))\n",
    "        g.add((transitionalSub, RDF.type, TransitionalDepositionalEnvironment))\n",
    "\n",
    "    ##Transitional Environment    \n",
    "    TransitionalDepositionalEnvironment = n['TransitionalDepositionalEnvironment']    \n",
    "    TransitionalDepositionalEnvironmentSub = ['BeachDepositionalEnvironment', 'DeltaicDepositionalEnvironment', 'LagoonalDepositionalEnvironment', 'TidalDepositionalEnvironment']\n",
    "    for className in TransitionalDepositionalEnvironmentSub:\n",
    "        className = n[className]\n",
    "        g.add((className, RDF.type, OWL.Class))\n",
    "        g.add((className, RDFS.subClassOf, TransitionalDepositionalEnvironment))  \n",
    "    \n",
    "        BeachDepositionalEnvironment = n['BeachDepositionalEnvironment']\n",
    "        BeachDepositionalEnvironmentSub = ['BackshoreDepositionalEnvironment', 'BeachDepositionalEnvironment', 'ForeshoreDepositionalEnvironment', 'LowerShorefaceDepositionalEnvironment', 'ShorefaceDepositionalEnvironment', 'UppershorefaceDepositionalEnvironment']\n",
    "        for item in BeachDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, BeachDepositionalEnvironment))   \n",
    "\n",
    "        DeltaicDepositionalEnvironment = n['DeltaicDepositionalEnvironment']\n",
    "        DeltaicDepositionalEnvironmentSub = ['Clinoforms', 'Delta', 'DeltaFrontDepositionalEnvironment', 'DeltaPlainDepositionalEnvironment', 'FluvioDeltaicDepositionalEnvironment', 'ProdeltaDepositionalEnvironment']\n",
    "        for item in DeltaicDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, DeltaicDepositionalEnvironment))   \n",
    "\n",
    "        LagoonalDepositionalEnvironment = n['LagoonalDepositionalEnvironment']\n",
    "        LagoonalDepositionalEnvironmentSub = ['LagoonalDepositionalEnv']\n",
    "        for item in LagoonalDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, LagoonalDepositionalEnvironment))   \n",
    "\n",
    "        TidalDepositionalEnvironment = n['TidalDepositionalEnvironment']\n",
    "        TidalDepositionalEnvironmentSub = ['SubtidalPlatform', 'TidalFlatDepositionalEnv', 'TidalPlatform']\n",
    "        for item in TidalDepositionalEnvironmentSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, TidalDepositionalEnvironment)) \n",
    "    \n",
    "GlacialDepositionalEnvironment = n['GlacialDepositionalEnvironment']\n",
    "GlacialDepositionalEnvironmentSub = ['GlacialDepositionalEnv']\n",
    "for item in GlacialDepositionalEnvironmentSub:\n",
    "    item = n[item]\n",
    "    g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "    g.add((item, RDF.type, GlacialDepositionalEnvironment)) \n",
    "\n",
    "VolcanicDepositionalEnvironment= n['VolcanicDepositionalEnvironment']\n",
    "VolcanicDepositionalEnvironmentSub = ['VolcanicDepositionalEnv']\n",
    "for item in VolcanicDepositionalEnvironmentSub:\n",
    "    item = n[item]\n",
    "    g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "    g.add((item, RDF.type, VolcanicDepositionalEnvironment)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matter class, fossil and rock subclasses and individuals\n",
    "\n",
    "Matter = n['Matter']\n",
    "g.add((Matter, RDF.type, OWL.Class))\n",
    "\n",
    "MatterSub = ['Fossil', 'Rock']\n",
    "for className in MatterSub:\n",
    "    className = n[className]\n",
    "    g.add((className, RDF.type, OWL.Class))\n",
    "    g.add((className, RDFS.subClassOf, Matter))\n",
    "\n",
    "    Fossil = n['Fossil']\n",
    "    FossilSub = ['CalcareousFossil', 'AgglutinatedFossil', 'SiliceousFossil', 'PhosphaticFossil', 'OrganicWalledFossil']\n",
    "    for className in FossilSub:\n",
    "        className = n[className]\n",
    "        g.add((className, RDF.type, OWL.Class))\n",
    "        g.add((className, RDFS.subClassOf, Fossil))\n",
    "\n",
    "        CalcareousFossil = n['CalcareousFossil']\n",
    "        CalcareousFossilSub = ['Foraminifera', 'Ostracods', 'CalcareousNannofossils']\n",
    "        for item in CalcareousFossilSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, CalcareousFossil))\n",
    "\n",
    "        AgglutinatedFossil = n['AgglutinatedFossil']\n",
    "        AgglutinatedFossilSub = ['AgglutinatedForaminifera']\n",
    "        for item in AgglutinatedFossilSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, AgglutinatedFossil))    \n",
    "\n",
    "        SiliceousFossil = n['SiliceousFossil']\n",
    "        SiliceousFossilSub = ['Chitinozoans', 'PollenSpores', 'Acritarchs', 'Dinoflagellates', 'Palynomorphs']\n",
    "        for item in SiliceousFossilSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, SiliceousFossil))  \n",
    "\n",
    "        PhosphaticFossil = n['PhosphaticFossil']\n",
    "        PhosphaticFossilSub = ['Conodonts']\n",
    "        for item in PhosphaticFossilSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, PhosphaticFossil)) \n",
    "\n",
    "        OrganicWalledFossil = n['OrganicWalledFossil']\n",
    "        OrganicWalledFossilSub = ['Radiolarians', 'Diatoms', 'Silicoflagellates']\n",
    "        for item in OrganicWalledFossilSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, OrganicWalledFossil))   \n",
    "\n",
    "    Rock = n['Rock']\n",
    "    RockSub = ['IgneousRock', 'MetamorphicRock', 'SedimentaryRock']\n",
    "    for className in RockSub:\n",
    "        className = n[className]\n",
    "        g.add((className, RDF.type, OWL.Class))\n",
    "        g.add((className, RDFS.subClassOf, Rock))\n",
    "\n",
    "        IgneousRock = n['IgneousRock']\n",
    "        IgneousRockSub = ['IgneousExtrusiveRock', 'IgneousIntrusiveRock']\n",
    "        for className in IgneousRockSub:\n",
    "            className = n[className]\n",
    "            g.add((className, RDF.type, OWL.Class))\n",
    "            g.add((className, RDFS.subClassOf, IgneousRock))\n",
    "\n",
    "            IgneousExtrusiveRock = n['IgneousExtrusiveRock']\n",
    "            IgneousExtrusiveRockSub = ['Andesite', 'AndesiteLava', 'Basalt', 'Dacite', 'Obsidian', 'Pumice', \n",
    "                                       'Rhyolite', 'Scoria', 'Tuff', 'VolcanicDeposits', 'VolcanicTuff']\n",
    "            for individual in IgneousExtrusiveRockSub:\n",
    "                individual = n[individual]\n",
    "                g.add((individual, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((individual, RDF.type, IgneousExtrusiveRock))\n",
    "\n",
    "            IgneousIntrusiveRock = n['IgneousIntrusiveRock']\n",
    "            IgneousIntrusiveRockSub = ['Diorite', 'Gabbro', 'Granite', 'Pegmatite', 'Peridotite']\n",
    "            for individual in IgneousIntrusiveRockSub:\n",
    "                individual = n[individual]\n",
    "                g.add((individual, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((individual, RDF.type, IgneousIntrusiveRock))\n",
    "\n",
    "        MetamorphicRock =n['MetamorphicRock']\n",
    "        MetamorphicRockSub = ['MetamorphicFoliatedRock', 'MetamorphicNonFoliatedRock']\n",
    "        for className in MetamorphicRockSub:\n",
    "            className = n[className]\n",
    "            g.add((className, RDF.type, OWL.Class))\n",
    "            g.add((className, RDFS.subClassOf, MetamorphicRock))\n",
    "\n",
    "        MetamorphicFoliatedRock =n['MetamorphicFoliatedRock']    \n",
    "        MetamorphicFoliatedRockSub = ['Gneiss', 'Phyllite', 'Schist', 'Slate']\n",
    "        for item in MetamorphicFoliatedRockSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, MetamorphicFoliatedRock))\n",
    "\n",
    "        MetamorphicNonFoliatedRock =n['MetamorphicNonFoliatedRock']\n",
    "        MetamorphicNonFoliatedRockSub = ['Marble', 'NonFoliatedRocks', 'Quartzite']\n",
    "        for item in MetamorphicNonFoliatedRockSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, MetamorphicNonFoliatedRock))\n",
    "\n",
    "        SedimentaryRock = n['SedimentaryRock']    \n",
    "        SedimentaryRockSub = ['Anhydrite', 'Breccia', 'Carbonate', 'Chalk', 'Chert', 'Coal', 'CoalyShale', 'Conglomerate',\n",
    "                              'Dolostone', 'Evaporites', 'Halite', 'Limestone', 'Marl', 'Mudstone', 'OrganicRichShale', \n",
    "                              'Salt', 'Sandstone', 'SedimentaryRocks', 'Shale', 'Siltstone', 'Spiculite']\n",
    "        for item in SedimentaryRockSub:\n",
    "            item = n[item]\n",
    "            g.add((item, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((item, RDF.type, SedimentaryRock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location class, GeoArea subclass and individuals\n",
    "\n",
    "Location = n['Location']\n",
    "g.add((Location, RDF.type, OWL.Class))\n",
    "\n",
    "LocationSub = ['GeoArea']\n",
    "for className in LocationSub:\n",
    "    className = n[className]\n",
    "    g.add((className, RDF.type, OWL.Class))\n",
    "    g.add((className, RDFS.subClassOf, Location))   \n",
    "    \n",
    "GeoArea = n['GeoArea']\n",
    "g.add((GeoArea, RDF.type, OWL.Class))\n",
    "\n",
    "GeoAreaSub = ['CentralNorthSea', 'SouthernNorthSea', 'NorthernNorthSea']\n",
    "for className in GeoAreaSub:\n",
    "    className = n[className]\n",
    "    g.add((className, RDF.type, OWL.NamedIndividual))\n",
    "    g.add((className, RDF.type, GeoArea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GeochronologicTime class\n",
    "\n",
    "GeochronologicTime = n['GeochronologicTime']\n",
    "g.add((GeochronologicTime, RDF.type, OWL.Class))\n",
    "\n",
    "GeochronologicTimeSub = ['PhanerozoicEon', 'GeologicalObject']\n",
    "for className in GeochronologicTimeSub:\n",
    "    className = n[className]\n",
    "    g.add((className, RDF.type, OWL.Class))\n",
    "    g.add((className, RDFS.subClassOf, GeochronologicTime)) \n",
    "\n",
    "    GeologicalObject = n['GeologicalObject']\n",
    "    GeologicalObjectSub = ['GeologicalUnit']\n",
    "    for className in GeologicalObjectSub:\n",
    "        className = n[className]\n",
    "        g.add((className, RDF.type, OWL.Class))\n",
    "        g.add((className, RDFS.subClassOf, GeologicalObject))\n",
    "        \n",
    "        GeologicalUnit = n['GeologicalUnit']\n",
    "        LithostratigraphicUnitSub = ['LithostratigraphicUnit']\n",
    "        for className in LithostratigraphicUnitSub:\n",
    "            className = n[className]\n",
    "            g.add((className, RDF.type, OWL.Class))\n",
    "            g.add((className, RDFS.subClassOf, GeologicalUnit))\n",
    "            g.add((depositedIn, OWL.someValuesFrom, DepositionalEnvironment)) \n",
    "    \n",
    "    PhanerozoicEon = n['PhanerozoicEon']\n",
    "    PhanerozoicEonSub = ['MesozoicEra', 'PaleozoicEra', 'CenozoicEra']\n",
    "    for className in PhanerozoicEonSub:\n",
    "        className = n[className]\n",
    "        g.add((className, RDF.type, OWL.Class))\n",
    "        g.add((className, RDFS.subClassOf, PhanerozoicEon))\n",
    "        \n",
    "        MesozoicEra = n['MesozoicEra']\n",
    "        MesozoicEraSub = ['CretaceousPeriod', 'JurassicPeriod', 'TriassicPeriod']\n",
    "        for className in MesozoicEraSub:\n",
    "            className = n[className]\n",
    "            g.add((className, RDF.type, OWL.Class))\n",
    "            g.add((className, RDFS.subClassOf, MesozoicEra))\n",
    "            \n",
    "            ##Cretaceous\n",
    "            CretaceousPeriod = n['CretaceousPeriod']\n",
    "            CretaceousPeriodSub = ['LowerCretaceousEpoch', 'UpperCretaceousEpoch']\n",
    "            Cretaceous = n['Cretaceous']\n",
    "            g.add((Cretaceous, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((Cretaceous, RDF.type, CretaceousPeriod))\n",
    "            \n",
    "            for className in CretaceousPeriodSub:\n",
    "                className = n[className]\n",
    "                g.add((className, RDF.type, OWL.Class))\n",
    "                g.add((className, RDFS.subClassOf, CretaceousPeriod))\n",
    "                \n",
    "                LowerCretaceous = n['LowerCretaceous']\n",
    "                LowerCretaceousEpoch = n['LowerCretaceousEpoch']\n",
    "                g.add((LowerCretaceous, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((LowerCretaceous, RDF.type, LowerCretaceousEpoch))\n",
    "                \n",
    "                UpperCretaceous = n['UpperCretaceous']\n",
    "                UpperCretaceousEpoch = n['UpperCretaceousEpoch']\n",
    "                g.add((UpperCretaceous, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((UpperCretaceous, RDF.type, UpperCretaceousEpoch))\n",
    "\n",
    "            JurassicPeriod = n['JurassicPeriod']\n",
    "            JurassicPeriodSub = ['LowerJurassicEpoch', 'MiddleJurassicEpoch', 'UpperJurassicEpoch']\n",
    "            Jurassic = n['Jurassic']\n",
    "            g.add((Jurassic, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((Jurassic, RDF.type, JurassicPeriod))\n",
    "            \n",
    "            for className in JurassicPeriodSub:\n",
    "                className = n[className]\n",
    "                g.add((className, RDF.type, OWL.Class))\n",
    "                g.add((className, RDFS.subClassOf, JurassicPeriod))\n",
    "                \n",
    "                LowerJurassic = n['LowerJurassic']\n",
    "                LowerJurassicEpoch = n['LowerJurassicEpoch']\n",
    "                g.add((LowerJurassic, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((LowerJurassic, RDF.type, LowerJurassicEpoch))\n",
    "                \n",
    "                MiddleJurassic = n['MiddleJurassic']\n",
    "                MiddleJurassicEpoch = n['MiddleJurassicEpoch']\n",
    "                g.add((MiddleJurassic, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((MiddleJurassic, RDF.type, MiddleJurassicEpoch))\n",
    "                \n",
    "                UpperJurassic = n['UpperJurassic']\n",
    "                UpperJurassicEpoch = n['UpperJurassicEpoch']\n",
    "                g.add((UpperJurassic, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((UpperJurassic, RDF.type, UpperJurassicEpoch))\n",
    "\n",
    "            TriassicPeriod = n['TriassicPeriod']\n",
    "            Triassic = n['Triassic']\n",
    "            g.add((Triassic, RDF.type, OWL.NamedIndividual))\n",
    "            g.add((Triassic, RDF.type, TriassicPeriod))\n",
    "            \n",
    "            TriassicPeriodSub = ['LowerTriassicEpoch', 'MiddleTriassicEpoch', 'UpperTriassicEpoch']\n",
    "            for className in TriassicPeriodSub:\n",
    "                className = n[className]\n",
    "                g.add((className, RDF.type, OWL.Class))\n",
    "                g.add((className, RDFS.subClassOf, TriassicPeriod))\n",
    "\n",
    "                LowerTriassic = n['LowerTriassic']\n",
    "                LowerTriassicEpoch = n['LowerTriassicEpoch']\n",
    "                g.add((LowerTriassic, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((LowerTriassic, RDF.type, LowerTriassicEpoch))\n",
    "\n",
    "                MiddleTriassic = n['MiddleTriassic']\n",
    "                MiddleTriassicEpoch = n['MiddleTriassicEpoch']\n",
    "                g.add((MiddleTriassic, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((MiddleTriassic, RDF.type, MiddleTriassicEpoch))\n",
    "\n",
    "                UpperTriassic = n['UpperTriassic']\n",
    "                UpperTriassicEpoch = n['UpperTriassicEpoch']\n",
    "                g.add((UpperTriassic, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((UpperTriassic, RDF.type, UpperTriassicEpoch))\n",
    "                \n",
    "        PaleozoicEra = n['PaleozoicEra']\n",
    "        PaleozoicEraSub = ['PermianPeriod', 'CarboniferousPeriod']\n",
    "        for className in PaleozoicEraSub:\n",
    "            className = n[className]\n",
    "            g.add((className, RDF.type, OWL.Class))\n",
    "            g.add((className, RDFS.subClassOf, PaleozoicEra))\n",
    "            \n",
    "            PermianPeriodSub = ['LowerPermian', 'UpperPermian', 'Permian']\n",
    "            PermianPeriod = n['PermianPeriod']\n",
    "            for indiName in PermianPeriodSub:\n",
    "                indiName = n[indiName]\n",
    "                g.add((indiName, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((indiName, RDF.type, PermianPeriod))\n",
    "                \n",
    "            CarboniferousPeriodSub = ['LowerCarboniferous', 'UpperCarboniferous', 'Carboniferous']\n",
    "            CarboniferousPeriod = n['CarboniferousPeriod']\n",
    "            for indiName in CarboniferousPeriodSub:\n",
    "                indiName = n[indiName]\n",
    "                g.add((indiName, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((indiName, RDF.type, CarboniferousPeriod))\n",
    "                \n",
    "        CenozoicEra = n['CenozoicEra']\n",
    "        CenozoicEraSub = ['NeogenePeriod', 'PaleogenePeriod', 'QuaternaryPeriod']\n",
    "        for className in CenozoicEraSub:\n",
    "            className = n[className]\n",
    "            g.add((className, RDF.type, OWL.Class))\n",
    "            g.add((className, RDFS.subClassOf, CenozoicEra))\n",
    "            \n",
    "            PaleogenePeriodSub = ['Paleocene', 'Eocene', 'Oligocene']\n",
    "            PaleogenePeriod = n['PaleogenePeriod']\n",
    "            for indiName in PaleogenePeriodSub:\n",
    "                indiName = n[indiName]\n",
    "                g.add((indiName, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((indiName, RDF.type, PaleogenePeriod))\n",
    "                \n",
    "            NeogenePeriodSub = ['Neogene']\n",
    "            NeogenePeriod = n['NeogenePeriod']\n",
    "            for indiName in NeogenePeriodSub:\n",
    "                indiName = n[indiName]\n",
    "                g.add((indiName, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((indiName, RDF.type, NeogenePeriod))\n",
    "                \n",
    "            QuaternaryPeriodSub = ['Quaternary']\n",
    "            QuaternaryPeriod = n['QuaternaryPeriod']\n",
    "            for indiName in QuaternaryPeriodSub:\n",
    "                indiName = n[indiName]\n",
    "                g.add((indiName, RDF.type, OWL.NamedIndividual))\n",
    "                g.add((indiName, RDF.type, QuaternaryPeriod))\n",
    "                \n",
    "           \n",
    "#Add hasDirectNext relationship\n",
    "g.add((LowerTriassic, hasDirectNext, MiddleTriassic))\n",
    "g.add((MiddleTriassic, hasDirectNext, UpperTriassic))\n",
    "g.add((UpperTriassic, hasDirectNext, LowerJurassic))\n",
    "g.add((LowerJurassic, hasDirectNext, MiddleJurassic))\n",
    "g.add((MiddleJurassic, hasDirectNext, UpperJurassic))\n",
    "g.add((UpperJurassic, hasDirectNext, LowerCretaceous))\n",
    "g.add((LowerCretaceous, hasDirectNext, UpperCretaceous))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Add classes and individuals to the knowledge graph from geolit_db.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import geolit_db.csv\n",
    "import csv\n",
    "reader = csv.DictReader(open('geolit_db.csv'))\n",
    "geodict = []\n",
    "for line in reader:\n",
    "    geodict.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that removed duplicates from a list\n",
    "def remove_dups_list(items):\n",
    "    return list(set(items))\n",
    "\n",
    "#Extracting LithostratographicUnit individuals from dataset\n",
    "lithoFm = []\n",
    "lithoGp = []\n",
    "\n",
    "for key in geodict:\n",
    "    lithoFm.append(key['lithofm'])\n",
    "    lithoGp.append(key['lithogp'])\n",
    "\n",
    "individuals = remove_dups_list(lithoFm) + remove_dups_list(lithoGp)\n",
    "#print(individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create individuals and their relationships\n",
    "\n",
    "for individual in individuals:\n",
    "    individual = n[individual]\n",
    "    g.add((individual, RDF.type, n['LithostratigraphicUnit']))\n",
    "    g.add((individual, RDF.type, OWL.NamedIndividual))\n",
    "\n",
    "for key in geodict:\n",
    "    lithoFm = n[key['lithofm']]\n",
    "    lithoGp = n[key['lithogp']]\n",
    "    geoArea = n[key['geoarea1']]\n",
    "    geoArea2 = n[key['geoarea2']]\n",
    "    chronoAge = n[key['chrono']]\n",
    "    rock1 = n[key['rock1']]\n",
    "    rock2 = n[key['rock2']]\n",
    "    rock3 = n[key['rock3']]\n",
    "    fossil1 = n[key['fossils1']]\n",
    "    fossil2 = n[key['fossils2']]\n",
    "    depoMain = n[key['depo']]\n",
    "    depo2 = n[key['depo2']]\n",
    "    depo3 = n[key['depo3']]\n",
    "    depo4 = n[key['depo4']]\n",
    "        \n",
    "    g.add((lithoFm, isMemberOf, lithoGp))\n",
    "    g.add((lithoFm, hasAge, chronoAge))\n",
    "    g.add((lithoFm, locatedIn, geoArea))\n",
    "    g.add((lithoFm, constitutedBy, rock1))\n",
    "    g.add((lithoFm, depositedIn, depoMain))\n",
    "    \n",
    "    \n",
    "    if key['rock2'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        g.add((lithoFm, constitutedBy, rock2))\n",
    "    \n",
    "    if key['rock3'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        g.add((lithoFm, constitutedBy, rock3))\n",
    "    \n",
    "    if key['fossils1'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        g.add((lithoFm, contains, fossil1))\n",
    "    \n",
    "    if key['fossils2'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        g.add((lithoFm, contains, fossil2))\n",
    "    \n",
    "    if key['depo2'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        g.add((lithoFm, depositedIn, depo2))\n",
    "    \n",
    "    if key['depo3'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        g.add((lithoFm, depositedIn, depo3))\n",
    "    \n",
    "    if key['depo4'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        g.add((lithoFm, depositedIn, depo4))\n",
    "        \n",
    "    if key['geoArea2'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        g.add((lithoFm, locatedIn, geoArea2))\n",
    "    \n",
    "    #if key['geoArea2'] == '':\n",
    "        #continue\n",
    "    #else:\n",
    "        #g.add((lithoFm, locatedIn, geoArea2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 1216  facts\n"
     ]
    }
   ],
   "source": [
    "# Size of graph\n",
    "print(f'Graph has {len(g)}  facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hash to view turtle formatted data of 1216 facts\n",
    "#print(g.serialize(format='ttl').decode('u8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hash to view triples\n",
    "#for index, (sub, pred, obj) in enumerate(g):\n",
    "    #print(sub, pred, obj)\n",
    "    #if index == 10:\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ttl format\n",
    "#g.serialize('geolit.ttl',format='ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Extract triples from turtle file and remove URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URI from triples using function\n",
    "# From https://github.com/ernestojimenezruiz/tabular-data-semantics-py/blob/master/TabularSemantics/src/util/utilities.py\n",
    "\n",
    "def getEntityName(uri):\n",
    "    \n",
    "    if \"#\" in uri:\n",
    "        splits = uri.split(\"#\")\n",
    "        if len(splits[1])>0:\n",
    "            return splits[1]\n",
    "        else:\n",
    "            return uri\n",
    "        \n",
    "    elif \"/\" in uri:\n",
    "        splits = uri.split(\"/\")\n",
    "        \n",
    "        i = len(splits)\n",
    "        \n",
    "        if len(splits[i-1])>0:\n",
    "            return splits[i-1]\n",
    "        else:\n",
    "            return uri\n",
    "        \n",
    "    return uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "geoT = []\n",
    "for index, (sub, pred, obj) in enumerate(g):\n",
    "    geoT.append((getEntityName(sub), getEntityName(pred), getEntityName(obj)))\n",
    "    \n",
    "#for row in geoTriples:\n",
    "geoTriples_df = pd.DataFrame(geoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop \"object, predicate\" rows for \"type, CLass\" and \"type, NamedIndividual\", along with other descriptors not required for triples data\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'Class') & (geoTriples_df.iloc[:,2] != 'Class')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'NamedIndividual') & (geoTriples_df.iloc[:,2] != 'NamedIndividual')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'hasNext') & (geoTriples_df.iloc[:,2] != 'hasNext')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'objectProperty') & (geoTriples_df.iloc[:,2] != 'objectProperty')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'isMemberOf') & (geoTriples_df.iloc[:,2] != 'isMemberOf')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'GeologicalObject') & (geoTriples_df.iloc[:,2] != 'GeologicalObject')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'hasDirectPrevious')] \n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'hasPrevious')] \n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'constitutedBy')] \n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'hasDirectNext')] \n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'depositedIn')] \n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'hasAge')] \n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'matter')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'GeoArea') & (geoTriples_df.iloc[:,2] != 'GeoArea')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'contains')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'location')]\n",
    "geoTriples_df = geoTriples_df[(geoTriples_df.iloc[:,0] != 'locatedIn')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 893 facts\n"
     ]
    }
   ],
   "source": [
    "# Size of new graph\n",
    "print(f'Graph has {len(geoTriples_df)} facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change format of triple as required by BERT\n",
    "geoTriples_df_formatted = geoTriples_df.replace(to_replace=r\"([A-Z])\", value= r\"_\\1\", regex=True)\n",
    "geoTriples_df_formatted[0] = geoTriples_df_formatted[0].str.lower()\n",
    "geoTriples_df_formatted[1] = geoTriples_df_formatted[1].str.lower()\n",
    "geoTriples_df_formatted[2] = geoTriples_df_formatted[2].str.lower()\n",
    "geoTriples_df_formatted[0] = geoTriples_df_formatted[0].map(lambda x: x.lstrip('_'))\n",
    "geoTriples_df_formatted[1] = geoTriples_df_formatted[1].map(lambda x: x.lstrip('_'))\n",
    "geoTriples_df_formatted[2] = geoTriples_df_formatted[2].map(lambda x: x.lstrip('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoTriples_df_formatted[0] = geoTriples_df_formatted[0].str.replace(r\"fm\", r\"formation\")\n",
    "geoTriples_df_formatted[0] = geoTriples_df_formatted[0].str.replace(r\"gp\", r\"group\")\n",
    "geoTriples_df_formatted[0] = geoTriples_df_formatted[0].str.replace(r\"mb\", r\"member\")\n",
    "geoTriples_df_formatted[0] = geoTriples_df_formatted[0].str.replace(r\"env\", r\"environment\")\n",
    "geoTriples_df_formatted[0] = geoTriples_df_formatted[0].str.replace(r\"environmentironment\", r\"environment\")\n",
    "geoTriples_df_formatted[1] = geoTriples_df_formatted[1].str.replace(r\"env\", r\"environment\")\n",
    "geoTriples_df_formatted[1] = geoTriples_df_formatted[1].str.replace(r\"environmentironment\", r\"environment\")\n",
    "geoTriples_df_formatted[2] = geoTriples_df_formatted[2].str.replace(r\"fm\", r\"formation\")\n",
    "geoTriples_df_formatted[2] = geoTriples_df_formatted[2].str.replace(r\"gp\", r\"group\")\n",
    "geoTriples_df_formatted[2] = geoTriples_df_formatted[2].str.replace(r\"mb\", r\"member\")\n",
    "geoTriples_df_formatted[2] = geoTriples_df_formatted[2].str.replace(r\"env\", r\"environment\")\n",
    "geoTriples_df_formatted[2] = geoTriples_df_formatted[2].str.replace(r\"environmentironment\", r\"environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Knowledge graph metrics\n",
    "Entity and relation density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density(triples, entities, relations):\n",
    "    ED = (2*triples) / entities\n",
    "    RD = triples / relations\n",
    "    print(f\"Entities density: {ED}\")\n",
    "    print(f\"Relations density: {RD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triples count: 893\n",
      "Unique entities: 289\n",
      "Unique realtions: 9\n"
     ]
    }
   ],
   "source": [
    "triples_count = len(geoTriples_df_formatted)\n",
    "\n",
    "uniqueRelations = len(geoTriples_df_formatted[1].unique())\n",
    "\n",
    "head_ents = geoTriples_df_formatted[0].unique()\n",
    "tail_ents = geoTriples_df_formatted[2].unique()\n",
    "entities = np.concatenate((head_ents, tail_ents))\n",
    "uniqueEntities = len(np.unique(entities))\n",
    "                     \n",
    "print(f\"Triples count: {triples_count}\")\n",
    "print(f\"Unique entities: {uniqueEntities}\")\n",
    "print(f\"Unique realtions: {uniqueRelations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities density: 6.179930795847751\n",
      "Relations density: 99.22222222222223\n"
     ]
    }
   ],
   "source": [
    "density(triples_count, uniqueEntities, uniqueRelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities density: 96.72592592592592\n",
      "Relations density: 141.93478260869566\n"
     ]
    }
   ],
   "source": [
    "# Density of UMLS dataset used for KG-BERT paper\n",
    "density(6529,135,46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities density: 4.543047651613218\n",
      "Relations density: 8454.818181818182\n"
     ]
    }
   ],
   "source": [
    "# Density of WN18RR dataset used for KG-BERT paper\n",
    "density(93003,40943,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities density: 42.65401279141737\n",
      "Relations density: 1308.506329113924\n"
     ]
    }
   ],
   "source": [
    "# Density of FB15k-237 dataset used for KG-BERT paper\n",
    "density(310116,14541,237)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Split data to be evaluated into train, validation, test\n",
    "Using ampligraph train_test_split_no_unseen functin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP\n",
    "# This will build new train, valid and test set for modelling, overwriting the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples: (893, 3)\n",
      "Size of train: (643, 3)\n",
      "Size of valid: (71, 3)\n",
      "Size of test: (179, 3)\n"
     ]
    }
   ],
   "source": [
    "import ampligraph\n",
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "\n",
    "test_valid, X_test = train_test_split_no_unseen(geoTriples_df_formatted.values, 179, seed=0) #Train/test split of 80/20 of total dataset\n",
    "\n",
    "X_train, X_valid = train_test_split_no_unseen(test_valid, 71, seed=0) #Validation set 10% of training set\n",
    "\n",
    "print('Total triples:', geoTriples_df_formatted.shape)\n",
    "print('Size of train:', X_train.shape)\n",
    "print('Size of valid:', X_valid.shape)\n",
    "print('Size of test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_valid = pd.DataFrame(X_valid)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tsv files with tab seperation \n",
    "X_train.to_csv('geolit_data/train.tsv', sep='\\t', index = False, header = False) \n",
    "X_valid.to_csv('geolit_data/dev.tsv', sep='\\t', index = False, header = False) \n",
    "X_test.to_csv('geolit_data/test.tsv', sep='\\t', index = False, header = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Extract entities and relations for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open train, valid and test datasets and extract unique entities list as text file\n",
    "\n",
    "with open('geolit_data/train.tsv', 'r') as f, open('geolit_data/test.tsv', 'r') as f1, open('geolit_data/dev.tsv', 'r') as f2, open('geolit_data/entities.txt', 'w') as f3:\n",
    "    lines = f.readlines() + f1.readlines() + f2.readlines()\n",
    "    entities = set()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        temp = line.split('\\t')\n",
    "        entities.add(temp[0])\n",
    "        entities.add(temp[2])\n",
    "    entities_str = '\\n'.join(list(entities))\n",
    "    f3.write(entities_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open train, valid and test datasets and extract unique relations list as text file\n",
    "\n",
    "with open('geolit_data/train.tsv', 'r') as f, open('geolit_data/test.tsv', 'r') as f1, open('geolit_data/dev.tsv', 'r') as f2, open('geolit_data/relations.txt', 'w') as f3:\n",
    "    relations = set()\n",
    "    lines = f.readlines() + f1.readlines() + f2.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        temp = line.split('\\t')\n",
    "        relations.add(temp[1])\n",
    "    relations_str = '\\n'.join(relations)\n",
    "    f3.write(relations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file with entities as space separated text, as required by BERT\n",
    "\n",
    "import re\n",
    "with open('geolit_data/entities.txt', 'r') as f, open('geolit_data/entity2text.txt', 'w') as f1:\n",
    "    lines = f.readlines()\n",
    "    ent2texts = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        ent2texts.append(line + '\\t' + line.replace('_', ' '))\n",
    "    f1.write('\\n'.join(ent2texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file with relations as space separated text, as required by BERT\n",
    "with open('geolit_data/relations.txt', 'r') as f, open('geolit_data/relation2text.txt', 'w') as f1:\n",
    "    lines = f.readlines()\n",
    "    relation_texts = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        #text = line.replace('/' , ' ')\n",
    "        text = line.replace('_', ' ').strip()\n",
    "        relation_texts.append(line + '\\t' + text)\n",
    "    f1.write('\\n'.join(relation_texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
